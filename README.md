# 基于因果推断的贝叶斯神经网络验证项目

这是一个示例性的小项目，展示了如何将因果推断、数据增广和贝叶斯深度学习结合起来，尝试在小数据场景下的效果。

## 项目结构

项目包含三个核心模块：

1. **数据生成模块** (`data_generator.py`)
   - 构造带有简单因果结构的合成数据集
   - 生成变量：X1、X2、X3和Y
   - 因果关系：X1 → X2 → Y，同时X1也直接影响Y
   - X3作为无关的噪声变量

2. **因果增广模块** (`causal_augmentation.py`)
   - 基于已知的因果结构，对数据进行"干预"增广
   - 通过对X1进行干预来生成新样本
   - 保持因果机制不变的情况下扩充数据集

3. **贝叶斯神经网络模块** (`bayesian_nn.py`)
   - 使用Pyro实现的贝叶斯神经网络
   - 网络结构：三层隐藏层 [64, 128, 64]，带批归一化和Dropout
   - 包含变分推断（SVI）训练过程
   - 支持不确定性估计的预测
   - 提供多种评估指标：MSE、MAE、R²分数
   - 包含基线模型对比（线性回归和普通MLP）

## 依赖要求

- Python 3.6+
- PyTorch
- Pyro-ppl
- NumPy
- Pandas
- scikit-learn

可以通过以下命令安装依赖：
```bash
pip install torch pyro-ppl numpy pandas scikit-learn
```

## 使用方法

1. 生成原始数据：
```python
from data_generator import generate_synthetic_data
df_original = generate_synthetic_data(n_samples=10000, seed=42)
```

2. 进行因果增广：
```python
from causal_augmentation import intervene_on_X1
df_aug = intervene_on_X1(df_original, x1_new_values=[-2, -1, 0, 1, 2])
```

3. 训练和评估模型：
```python
python bayesian_nn.py
```

## 最新优化

1. **数据预处理优化**
   - [x] 添加数据分析功能，自动检测是否需要标准化
   - [x] 实现自动标准化处理（使用StandardScaler）
   - [x] 增加数据统计特征分析和可视化

2. **模型架构优化**
   - [x] 增加网络规模（[64, 128, 64]）
   - [x] 添加Dropout层（比例0.1）防止过拟合
   - [x] 使用批归一化提升训练稳定性
   - [x] 添加残差连接

3. **训练过程优化**
   - [x] 实现动态学习率调整（每1000轮减半）
   - [x] 添加早停机制（patience=10）
   - [x] 放宽先验分布（N(0, 5)）
   - [x] 实现可学习的噪声参数sigma

4. **评估方法优化**
   - [x] 添加基线模型对比
     * 线性回归作为简单基线
     * 普通MLP作为神经网络基线
   - [x] 提供更多评估指标
     * 均方误差 (MSE)
     * 平均绝对误差 (MAE)
     * 决定系数 (R²)
     * 预测不确定性统计

## 注意事项

- 当前实现已经过多项优化：
  * 更大的数据集（10000样本）
  * 更复杂的网络结构（三层神经网络）
  * 更稳定的训练过程（动态学习率、早停）
  * 更全面的评估体系（多个基线模型和评估指标）
- 模型架构包含多个现代深度学习技巧：
  * Dropout防止过拟合
  * 批归一化提升训练稳定性
  * 动态学习率调整
- 提供了完整的数据分析和预处理流程
- 支持自动化的模型评估和比较

## 版本历史

2024-12-28 1.0.0 初始版本
2024-12-29 1.1.0 添加数据预处理和模型优化

## 测试结果及结论

### 测试结果
特征统计:
                 X1            X2            X3             Y
count  60000.000000  60000.000000  60000.000000  60000.000000
mean      -0.003894      0.000139     -0.001790     -0.008513
std        1.373815      0.715375      0.990575      3.794599
min       -2.000000     -1.859078     -3.856375     -7.170271
25%       -1.000000     -0.592748     -0.686030     -2.971168
50%        0.000000      0.002436     -0.000148     -0.000297
75%        1.000000      0.594934      0.674377      2.964142
max        2.000000      1.753247      3.942331      6.985676

数据范围:
X1: [-2.00, 2.00], 范围: 4.00
X2: [-1.86, 1.75], 范围: 3.61
X3: [-3.86, 3.94], 范围: 7.80
Y: [-7.17, 6.99], 范围: 14.16

训练基线模型...

训练贝叶斯神经网络...
Epoch [200/5000] - Loss: 458245.961, Sigma: 1.221
Epoch [400/5000] - Loss: 356015.213, Sigma: 1.492
Epoch [600/5000] - Loss: 290693.607, Sigma: 1.822
Epoch [800/5000] - Loss: 249628.355, Sigma: 2.226
Epoch [1000/5000] - Loss: 224831.514, Sigma: 2.718

学习率调整为: 0.0005
Epoch [1200/5000] - Loss: 211190.471, Sigma: 3.320
Epoch [1400/5000] - Loss: 204666.802, Sigma: 4.055
Epoch [1600/5000] - Loss: 201047.366, Sigma: 4.079
Early stopping at epoch 1666

模型评估...

线性回归 评估结果:
MSE: 0.0404
MAE: 0.1607
R²: 0.9960

简单MLP 评估结果:
MSE: 0.0411
MAE: 0.1622
R²: 0.9959

贝叶斯神经网络 评估结果:
MSE: 12.9441
MAE: 3.0136
R²: -0.2812
平均预测不确定性: 4.2666 ± 1.0126

部分预测结果示例 (贝叶斯神经网络):
 TrueY=-1.670, Pred=1.583 ± 3.819
 TrueY=4.622, Pred=1.952 ± 4.025
 TrueY=2.800, Pred=2.158 ± 4.008
 TrueY=0.530, Pred=2.135 ± 3.940
 TrueY=-3.421, Pred=1.576 ± 4.057

### 结论
 1. Loss 依然很大，逐渐变小的过程伴随𝜎不断变大，说明模型依然没有学到真正的因果关系。BNN 在学不到有效映射时，自适应地把观测噪声学得极大，让自己“什么都不用学，也能解释数据”，导致最终预测也变得非常不稳定。
 2. BNN 几乎把所有输入都映射到同一个“常值区间”，再加上一个大方差来掩饰差距——典型的“过强先验 + 欠拟合 + 大𝜎”模式。
 3. 数据本身很容易被学到，而 BNN 却学不到，说明BNN 的训练过程或先验/推断限制过于强，或算法实现还有漏洞。